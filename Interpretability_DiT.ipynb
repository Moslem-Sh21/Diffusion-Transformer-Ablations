{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install diffusers torch requests pillow tqdm\n",
    "!pip install pycocotools\n",
    "!pip install torch-fidelity\n",
    "!pip install openai-clip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q val2017.zip\n",
    "!mkdir -p annotations\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q annotations_trainval2017.zip -d annotations"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "antsyI6NVknX",
    "outputId": "25b046aa-6206-488a-8ae3-eb768957072e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.32.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
      "Collecting torch-fidelity\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (2.0.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (11.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.15.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (0.21.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch-fidelity) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
      "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: torch-fidelity\n",
      "Successfully installed torch-fidelity-0.3.0\n",
      "Collecting openai-clip\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy (from openai-clip)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=74ea0e99fa676234032cd74f5220d86281758ff332a887d693ac3289674da8b5\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: ftfy, openai-clip\n",
      "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n",
      "--2025-06-13 02:31:21--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.124.89, 52.216.114.131, 16.182.73.201, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.124.89|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: \u2018val2017.zip\u2019\n",
      "\n",
      "val2017.zip         100%[===================>] 777.80M  80.5MB/s    in 8.6s    \n",
      "\n",
      "2025-06-13 02:31:30 (90.2 MB/s) - \u2018val2017.zip\u2019 saved [815585330/815585330]\n",
      "\n",
      "--2025-06-13 02:31:37--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.37, 54.231.201.193, 3.5.28.233, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.37|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: \u2018annotations_trainval2017.zip\u2019\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.19M  96.4MB/s    in 2.5s    \n",
      "\n",
      "2025-06-13 02:31:39 (96.4 MB/s) - \u2018annotations_trainval2017.zip\u2019 saved [252907541/252907541]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch_fidelity import calculate_metrics\n",
    "from diffusers import PixArtAlphaPipeline\n",
    "import clip\n",
    "# COCO paths (set your own path if not in current dir)\n",
    "COCO_ANN_PATH = \"./annotations/captions_val2017.json\"\n",
    "COCO_IMG_DIR = \"./val2017\"  # Folder containing val2017/*.jpg\n",
    "\n",
    "N = 5  # Number of random samples to generate\n",
    "\n",
    "# 1. Download captions file if not exists\n",
    "if not os.path.exists(COCO_ANN_PATH):\n",
    "    url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    print(\"Downloading COCO annotations...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(\"annotations_trainval2017.zip\", \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"annotations_trainval2017.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    os.remove(\"annotations_trainval2017.zip\")\n",
    "\n",
    "# 2. Load COCO captions\n",
    "import json\n",
    "with open(COCO_ANN_PATH, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Build (img_path, caption) pairs\n",
    "id2filename = {img['id']: img['file_name'] for img in coco['images']}\n",
    "data = []\n",
    "for ann in coco['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    img_path = os.path.join(COCO_IMG_DIR, id2filename[img_id])\n",
    "    if os.path.exists(img_path):\n",
    "        data.append((img_path, caption))\n",
    "print(f\"Loaded {len(data)} (image, caption) pairs from COCO val2017.\")\n",
    "\n",
    "# 3. Pick N random samples\n",
    "chosen = random.sample(data, N)\n",
    "\n",
    "def ablate_dit_part(dit, blocks_to_patch, part='ff', mode='zero'):\n",
    "    \"\"\"\n",
    "    Patch a given part ('attn1', 'attn2', 'ff') of DiT blocks in PixArt-\u03b1 (diffusers).\n",
    "    part: 'attn1' (self-attn), 'attn2' (cross-attn), 'ff' (MLP)\n",
    "    mode: 'zero' | 'input' | 'mean'\n",
    "    \"\"\"\n",
    "    for idx, block in enumerate(dit.transformer_blocks):\n",
    "        if idx not in blocks_to_patch:\n",
    "            continue\n",
    "        sub = getattr(block, part)\n",
    "        orig_forward = sub.forward\n",
    "\n",
    "        def ablated_forward(self, x, *args, **kwargs):\n",
    "            if mode == 'zero':\n",
    "                return torch.zeros_like(x)\n",
    "            elif mode == 'input':\n",
    "                return x\n",
    "            elif mode == 'mean':\n",
    "                return x.mean(dim=-1, keepdim=True).expand_as(x)\n",
    "            return orig_forward(x, *args, **kwargs)\n",
    "        sub.forward = ablated_forward.__get__(sub, sub.__class__)\n",
    "\n",
    "# -- Setup --\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe = PixArtAlphaPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-XL-2-512x512\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.to(device)\n",
    "# print(dir(pipe.transformer))\n",
    "# block = pipe.transformer.transformer_blocks[0]\n",
    "# print(block)\n",
    "# print(dir(block))\n",
    "\n",
    "# Load CLIP\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "output_size = (299, 299)  # For FID/CLIP\n",
    "os.makedirs(\"./pixart_samples/real\", exist_ok=True)\n",
    "os.makedirs(\"./pixart_samples/fake\", exist_ok=True)\n",
    "os.makedirs(\"./pixart_samples/ablated\", exist_ok=True)\n",
    "\n",
    "# -- Normal Generation (Baseline) --\n",
    "for i, (img_path, prompt) in enumerate(tqdm(chosen, desc=\"Generating original\")):\n",
    "    gt_img = Image.open(img_path).convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
    "    gt_img.save(f\"./pixart_samples/real/gt_{i+1}.jpg\")\n",
    "    gen_img = pipe(prompt).images[0].convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
    "    gen_img.save(f\"./pixart_samples/fake/gen_{i+1}.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Shut down (mean-ablate) layer 1 and 2\n",
    "# patch_dit_layers(pipe.transformer, layers_to_ablate=[0], mode='input')\n",
    "# ablate_dit_part(pipe.transformer, blocks_to_patch=[5,8], part='attn1', mode='input')\n",
    "# ablate_dit_part(pipe.transformer, blocks_to_patch=[11], part='ff', mode='zero')\n",
    "ablate_dit_part(pipe.transformer, blocks_to_patch=list(range(12)), part='attn2', mode='input')\n",
    "print(len(pipe.transformer.transformer_blocks))\n",
    "for i, (img_path, prompt) in enumerate(tqdm(chosen, desc=\"Generating ablated\")):\n",
    "    ablated_img = pipe(prompt).images[0].convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
    "    ablated_img.save(f\"./pixart_samples/ablated/ablated_{i+1}.jpg\")\n",
    "\n",
    "# -- FID Calculation --\n",
    "print(\"FID (Original vs Real):\")\n",
    "fid_orig = calculate_metrics(\n",
    "    input1=\"./pixart_samples/real\",\n",
    "    input2=\"./pixart_samples/fake\",\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    isc=False, kid=False, fid=True, verbose=True,\n",
    ")\n",
    "print(\"FID (Ablated vs Real):\")\n",
    "fid_ablate = calculate_metrics(\n",
    "    input1=\"./pixart_samples/real\",\n",
    "    input2=\"./pixart_samples/ablated\",\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    isc=False, kid=False, fid=True, verbose=True,\n",
    ")\n",
    "print(f\"Original FID: {fid_orig['frechet_inception_distance']}\")\n",
    "print(f\"Ablated FID: {fid_ablate['frechet_inception_distance']}\")\n",
    "\n",
    "# -- CLIP Score Calculation --\n",
    "def compute_clip_scores(img_dir, captions):\n",
    "    scores = []\n",
    "    for i, (_, prompt) in enumerate(captions):\n",
    "        img = Image.open(f\"{img_dir}/gen_{i+1}.jpg\").convert(\"RGB\")\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(img_tensor)\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (image_features @ text_features.T).item()\n",
    "        scores.append(similarity)\n",
    "    return scores\n",
    "\n",
    "print(\"CLIP Score (Original):\")\n",
    "orig_clip_scores = compute_clip_scores(\"./pixart_samples/fake\", chosen)\n",
    "print(\"CLIP Score (Ablated):\")\n",
    "def compute_clip_scores_ablated(img_dir, captions):\n",
    "    scores = []\n",
    "    for i, (_, prompt) in enumerate(captions):\n",
    "        img = Image.open(f\"{img_dir}/ablated_{i+1}.jpg\").convert(\"RGB\")\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(img_tensor)\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (image_features @ text_features.T).item()\n",
    "        scores.append(similarity)\n",
    "    return scores\n",
    "ablated_clip_scores = compute_clip_scores_ablated(\"./pixart_samples/ablated\", chosen)\n",
    "\n",
    "print(f\"Mean CLIP (Original): {sum(orig_clip_scores)/len(orig_clip_scores):.4f}\")\n",
    "print(f\"Mean CLIP (Ablated): {sum(ablated_clip_scores)/len(ablated_clip_scores):.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bad320e9d5654039bcd79eda2a1d4523",
      "7a5b0652aaa94db9b5d30795c4661792",
      "2e1991a6dae8419ba7172b69d0b57233",
      "4cb4679db62c4e1fbf711825e617e380",
      "1838fba0c4994dbb94e6fc60f33eea38",
      "8e131425591c4673a3b8cdb7a1733bab",
      "b72b2ba3589e4ec298bb315a861eedca",
      "d2a45d43ed35427283577c28d47de6d6",
      "fac5729ea0f745efa64e1c68c451c343",
      "77e4050b1cfe44c8ba62f20d858ffc52",
      "9dd738ddb8b044088901734a4b9b267c",
      "b53255261cf24d7183be654d5d32703f",
      "8c4260180b5a47298081bc00d7642359",
      "1e577e034a2946c88df68e2491d25a03",
      "8a9ce06f608840068647641f0c5b7089",
      "c2be36f96cfe4e7b8b737914dddceea4",
      "fddb17ddfd17448b806b8ff84c79f07a",
      "b3e4c8879aca41c7b9ade30ee1ab5632",
      "ff92ff95f300450194b0e7a754bfeb88",
      "9c6a571761e545e290cd9e765437d6d9",
      "8bc866e3b47e4cd9aadd07e8e6f0ff87",
      "96c7c5b5796b4d438ba3575814be65c0",
      "5d52c5f1207a442d947905dcf114a826",
      "3b145b7d348d4b97b0ae6310ac8014da",
      "1e60782f40674e0caab0249d294ede5a",
      "eaf3c67fcf0544f1a9388cf19d555d0c",
      "a9c42636d90d494db5a46d01b8692ffa",
      "6e21e2da39e149b49568ad55109ca52e",
      "11e2a8c0fb934a3ea1bf096a49fe8530",
      "3f27aa09eb854744b3d83faf9cbb742e",
      "af554d4ade064932848369440dafcc2c",
      "44f452536c514ca0a95428297f298ba9",
      "97a42be18e104486828cf5c77606bfed",
      "3b4a3c9f7aa24e71a35b3608a2846723",
      "f7429d42288045b2b00dcb9c8df81ead",
      "2e63aa8c63f944ba8e2f74dff0ead7f9",
      "72e52a0c7d0347349cf86ed3b4c2f94c",
      "c8a8b791e636495094993679728f65f1",
      "b60c5d90fff44564ba51877ead331d15",
      "413ad9130733412b946ae91163d26aaf",
      "eed7466f76cd4f7eacae86df0a84ff25",
      "e6461b592ad64587b058021f19179bec",
      "46dfdf4db36a45fbb5691ef2ba8d4035",
      "a7bcc6808ae04284bc442693c020045f",
      "6aed5b98f1c8430da78642716df017ee",
      "573a27cd9a604ced806d3d0939d6eeed",
      "15b2bf2194474b8d98ded3f7d0a71848",
      "ec7a68abda264877b11cb1920b829482",
      "5b8f8331b7c14e2d86e6171d5d9a0c7a",
      "6c91e95160f145a2ad639e6866ab1e37",
      "674dc3a1f97b483d81ca791ee9eb39e0",
      "78c1a74841134601ad5dbd3fbea5c4e4",
      "1c502be41922469c9c8776c5464de64a",
      "c1ca7b4a46164ff8b9cda51549b097a7",
      "b7adc7ee4c694b1e855c8736963b6484",
      "fb2e8ca493e146038cd4ac90ead2595e",
      "5945349919ba47eaabca1dba0fd92519",
      "0fce25a859ed4b4abf00698aa04544c1",
      "224ddb5c07f5491ab3248923ebbff515",
      "885d454793d448a58526537d5679c01b",
      "f43343822b23458e9804c357abf4b923",
      "f82645d5398b4b9490b5881e17904ec2",
      "77ac9eaa3920472698b4c369f647ffb5",
      "a4b00536a7874513a56ad7e5ec4eb1b6",
      "339af3368c0f44f09533ae5953fb4741",
      "3873b13bafdc4ed3899417232727e022",
      "ddeb46d4b48449f4891fb953fadc5d4c",
      "f5b2b3d5910146488d58fa8f38d357e6",
      "a23b04dba0f943899e46088fece6640b",
      "20e0d70bc9aa4728b9ab55d9f3c3ff4f",
      "5408fc0d2dfa49079238015da6f91d88",
      "c003261249944c45adb08e4073affb5d",
      "5317ab63fe9841a580326ccecdc750f0",
      "93a9ee87ab114d89aaedd6abdb0bd713",
      "80a74e28840445f0991df985a2dc0e0b",
      "1f87ddaf2daf425f96e9ceb9251a0d2a",
      "2f4481324d2b4c04ae3bb8e7c4afe110",
      "136e2bcf17464ce0a619a7723c1c057d",
      "53014c7fc6e342fb9c9fdb92629653fe",
      "6da56df33c554c85a6963839adebec70",
      "56f0b726b40344bcbf2cc4924a2a6e2c",
      "d18192557bfa427a91b2c2ea712c120b",
      "d090cfb15929451fa6d6babbb2d71cd0",
      "0e1f7d4522b649bc99bd900195bda620",
      "75475839e4074ff4917b7e9f99b4ca78",
      "923c8b3fb66048a6a581e5311f966b03",
      "0c278625b54146b19364422aa8f52907",
      "4756d94e80a14852bde30d15fada8918",
      "c41a469002b54cca8cb7aca1653f6626",
      "478a29cd32024b3db56952f2b5f3e6a6",
      "a6b7e1224faa4a57a1f41838f5f9706c",
      "0d36748b3aa44be28181f4abf8a7bda5",
      "4c6a0a3db93748b381a69a61305c4483",
      "78db978be89148f687519692e9de3335",
      "25a70f64d5414c2696548e2fc9ca23cf",
      "a76376d3d5324879b67c6487acb17a94",
      "1712a636cfeb4d6db449168ef82263d6",
      "8f7fb5ec5cf6429b8ac3c1d3eb014ca1",
      "e5cd080a084f4ad58c26bb672a6c2140",
      "8da38c787ed54439ba55e35110286ba1",
      "040b0d8c50994b40b4b81e8cbdf71728",
      "e5467c96cad144cc9d2a82f3527bd6d9",
      "44313059ae7d44069814fea398ebc2a6",
      "3dfc316dfee2442fa3925cd4bf7d57b0",
      "85964a52e90a418b8a2ddd067ece5e02",
      "2fadccd9fddc48b58d0a6947fff36873",
      "c2a32b04037a409eabf1a2ed547ab679",
      "1f4d8798730344699c770d23612940f3",
      "a988418ce4b443b3ad80c45d371abd6d",
      "175e33c01ea7435b835510aceb95c6a9",
      "adae4e1ddb0e470ca9836a74f0fc93b0",
      "7b562f6d5a2c4c749f9336dac0d668a0",
      "1fd339f087c1492aa49da042bf9007d8",
      "363e3e7e92e84bb982f25240a1fa9d58",
      "fa004642ecd84f1f98cb0e7ed72f957a",
      "a1892cc9556c49979545995cd9c8cf5d",
      "cf808aa0584b491d9a1bedb3b25837f4",
      "83fa0bdb17bd405eae596362848b9d6b",
      "cba7d9c1b1844e148b46e3acf044db02",
      "4bf1d4ee20dc4e34befbad44d17dcd56",
      "4e0bbef295c64fb1831e4937aaa02ef2",
      "23f4d5aae5e847bda7fdc6b420484863",
      "84a171154f72479cb2dcc1fb53a0d6f1",
      "34a61553c3ab4f5cac1c7638cea880b0",
      "8b0894a60c5241d39d4a9ed788be8fb7",
      "f259b4e6c2684f7e9a35e5e2f1335064",
      "91f205477e964622aa6bbbcac5c1ce73",
      "ede333c5ee124e36a838a97d580518fa",
      "f84539b9bec44e659f1fa7c5b2e6fd6a",
      "2481b560061749c6a24f672771fde42e",
      "ca12f3c5b0d0414694f97b69c017d893",
      "4439259321764f86bef95db0715953ec"
     ]
    },
    "id": "CqlWhcOqS8O0",
    "outputId": "cf322d4c-40e9-4965-b8a9-6ce4c8e3ce89"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 25014 (image, caption) pairs from COCO val2017.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bad320e9d5654039bcd79eda2a1d4523"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at /root/.cache/huggingface/hub/models--PixArt-alpha--PixArt-XL-2-512x512/snapshots/50f702106901db6d0f8b67eb88e814c56ded2692/transformer were not used when initializing PixArtTransformer2DModel: \n",
      " ['caption_projection.y_embedding']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b53255261cf24d7183be654d5d32703f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating original:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d52c5f1207a442d947905dcf114a826"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating original:  20%|\u2588\u2588        | 1/5 [00:01<00:04,  1.06s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b4a3c9f7aa24e71a35b3608a2846723"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating original:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:02<00:03,  1.05s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6aed5b98f1c8430da78642716df017ee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating original:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:03<00:02,  1.05s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb2e8ca493e146038cd4ac90ead2595e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating original:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:04<00:01,  1.05s/it]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddeb46d4b48449f4891fb953fadc5d4c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generating original: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "28\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating ablated:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "136e2bcf17464ce0a619a7723c1c057d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/diffusers/image_processor.py:147: RuntimeWarning: invalid value encountered in cast\n",
      "  images = (images * 255).round().astype(\"uint8\")\n",
      "\rGenerating ablated:  20%|\u2588\u2588        | 1/5 [00:00<00:03,  1.09it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c41a469002b54cca8cb7aca1653f6626"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating ablated:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:01<00:02,  1.09it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8da38c787ed54439ba55e35110286ba1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating ablated:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:02<00:01,  1.09it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adae4e1ddb0e470ca9836a74f0fc93b0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating ablated:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23f4d5aae5e847bda7fdc6b420484863"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generating ablated: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:04<00:00,  1.10it/s]\n",
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FID (Original vs Real):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./pixart_samples/real\" with extensions png,jpg,jpeg\n",
      "Found 5 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples\n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./pixart_samples/fake\" with extensions png,jpg,jpeg\n",
      "Found 5 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples\n",
      "Frechet Inception Distance: 308.2989888639742\n",
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FID (Ablated vs Real):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./pixart_samples/real\" with extensions png,jpg,jpeg\n",
      "Found 5 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples\n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./pixart_samples/ablated\" with extensions png,jpg,jpeg\n",
      "Found 5 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/linalg/_matfuncs_sqrtm.py:200: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  arg2 = norm(X.dot(X) - A, 'fro')**2 / norm(A, 'fro')\n",
      "Frechet Inception Distance: 748.0256444859742\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original FID: 308.2989888639742\n",
      "Ablated FID: 748.0256444859742\n",
      "CLIP Score (Original):\n",
      "CLIP Score (Ablated):\n",
      "Mean CLIP (Original): 0.3205\n",
      "Mean CLIP (Ablated): 0.2127\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch_fidelity import calculate_metrics\n",
    "from diffusers import PixArtAlphaPipeline\n",
    "import clip\n",
    "# COCO paths (set your own path if not in current dir)\n",
    "COCO_ANN_PATH = \"./annotations/captions_val2017.json\"\n",
    "COCO_IMG_DIR = \"./val2017\"  # Folder containing val2017/*.jpg\n",
    "\n",
    "def patch_dit_layers(dit, layers_to_ablate, mode='zero'):\n",
    "    \"\"\"\n",
    "    Patch the forward method of the DiT's transformer layers in diffusers PixArt-\u03b1.\n",
    "    \"\"\"\n",
    "    for idx, block in enumerate(dit.transformer_blocks):\n",
    "        orig_forward = block.forward\n",
    "\n",
    "        def ablated_forward(self, x, *args, **kwargs):\n",
    "            if idx in layers_to_ablate:\n",
    "                if mode == 'zero':\n",
    "                    return torch.zeros_like(x)\n",
    "                elif mode == 'input':\n",
    "                    return x\n",
    "                elif mode == 'mean':\n",
    "                    # Mean over (batch, spatial, channel)\n",
    "                    return x.mean(dim=[1, 2], keepdim=True).expand_as(x)\n",
    "            return orig_forward(x, *args, **kwargs)\n",
    "        block.forward = ablated_forward.__get__(block, block.__class__)\n",
    "N = 5  # Number of random samples to generate\n",
    "\n",
    "# 1. Download captions file if not exists\n",
    "if not os.path.exists(COCO_ANN_PATH):\n",
    "    url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    print(\"Downloading COCO annotations...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(\"annotations_trainval2017.zip\", \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"annotations_trainval2017.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    os.remove(\"annotations_trainval2017.zip\")\n",
    "\n",
    "# 2. Load COCO captions\n",
    "import json\n",
    "with open(COCO_ANN_PATH, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Build (img_path, caption) pairs\n",
    "id2filename = {img['id']: img['file_name'] for img in coco['images']}\n",
    "data = []\n",
    "for ann in coco['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    img_path = os.path.join(COCO_IMG_DIR, id2filename[img_id])\n",
    "    if os.path.exists(img_path):\n",
    "        data.append((img_path, caption))\n",
    "print(f\"Loaded {len(data)} (image, caption) pairs from COCO val2017.\")\n",
    "\n",
    "# 3. Pick N random samples\n",
    "chosen = random.sample(data, N)\n",
    "\n",
    "# 4. Load PixArt-\u03b1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe = PixArtAlphaPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-XL-2-512x512\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "# 5. Generate images\n",
    "os.makedirs(\"./pixart_samples/real\", exist_ok=True)\n",
    "os.makedirs(\"./pixart_samples/fake\", exist_ok=True)\n",
    "\n",
    "output_size = (299, 299)  # for FID\n",
    "\n",
    "for i, (img_path, prompt) in enumerate(tqdm(chosen, desc=\"Generating\")):\n",
    "    # Ground truth\n",
    "    gt_img = Image.open(img_path).convert(\"RGB\")\n",
    "    gt_img = gt_img.resize(output_size, Image.LANCZOS)\n",
    "    gt_img.save(f\"./pixart_samples/real/gt_{i+1}.jpg\")\n",
    "\n",
    "    # Generated\n",
    "    gen_img = pipe(prompt).images[0].convert(\"RGB\")\n",
    "    gen_img = gen_img.resize(output_size, Image.LANCZOS)\n",
    "    gen_img.save(f\"./pixart_samples/fake/gen_{i+1}.jpg\")\n",
    "    print(f\"Saved: gt_{i+1}.jpg (ground truth), gen_{i+1}.jpg (generated)\")\n",
    "\n",
    "for folder in [\"./pixart_samples/real\", \"./pixart_samples/fake\"]:\n",
    "    for fname in os.listdir(folder):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        if os.path.getsize(fpath) == 0:\n",
    "            print(f\"Removing zero-byte file: {fpath}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "-- FID Calculation --\n",
    "\n",
    "metrics = calculate_metrics(\n",
    "    input1=\"./pixart_samples/real\",\n",
    "    input2=\"./pixart_samples/fake\",\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    isc=False,\n",
    "    kid=False,\n",
    "    fid=True,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"FID between generated and ground-truth: {metrics['frechet_inception_distance']}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "KaHbo7fDRiiE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import clip\n",
    "\n",
    "# Load CLIP\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "clip_scores = []\n",
    "for i, (img_path, prompt) in enumerate(chosen):\n",
    "    # Load generated image\n",
    "    gen_img = Image.open(f\"./pixart_samples/fake/gen_{i+1}.jpg\").convert(\"RGB\")\n",
    "    gen_img = preprocess(gen_img).unsqueeze(0).to(device)\n",
    "    # Encode prompt\n",
    "    text = clip.tokenize([prompt]).to(device)\n",
    "\n",
    "    # CLIP similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(gen_img)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).item()  # Cosine similarity\n",
    "\n",
    "    print(f\"Sample {i+1}: CLIP score = {similarity:.4f} | Caption: {prompt}\")\n",
    "    clip_scores.append(similarity)\n",
    "\n",
    "print(f\"\\nMean CLIP score over {len(clip_scores)} samples: {sum(clip_scores)/len(clip_scores):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wn9K7bTUpbp5",
    "outputId": "97bd7588-7a1b-499b-9578-2ed0d8b8947e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 338M/338M [00:09<00:00, 35.9MiB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample 1: CLIP score = 0.2825 | Caption: A plate of some food on a table.\n",
      "Sample 2: CLIP score = 0.3010 | Caption: A plate of donuts with a person in the background.\n",
      "Sample 3: CLIP score = 0.3057 | Caption: A picture of a small-sized kitchen with wood cabinets.\n",
      "Sample 4: CLIP score = 0.3071 | Caption: a couple of boats sitting on top of a body of water.\n",
      "Sample 5: CLIP score = 0.3213 | Caption: A very small boy on the beach with a disc.\n",
      "\n",
      "Mean CLIP score over 5 samples: 0.3035\n"
     ]
    }
   ]
  }
 ]
}