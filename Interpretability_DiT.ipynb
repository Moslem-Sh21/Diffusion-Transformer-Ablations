{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers torch requests pillow tqdm\n",
        "!pip install pycocotools\n",
        "!pip install torch-fidelity\n",
        "!pip install openai-clip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip -q val2017.zip\n",
        "!mkdir -p annotations\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -q annotations_trainval2017.zip -d annotations"
      ],
      "metadata": {
        "id": "antsyI6NVknX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch_fidelity import calculate_metrics\n",
        "from diffusers import PixArtAlphaPipeline\n",
        "import clip\n",
        "import json\n",
        "\n",
        "# COCO paths\n",
        "COCO_ANN_PATH = \"annotations/annotations/captions_val2017.json\"\n",
        "COCO_IMG_DIR = \"./val2017\"\n",
        "N = 5  # Number of random samples\n",
        "\n",
        "def ablate_dit_part(dit, blocks_to_patch, part='ff', mode='zero'):\n",
        "    \"\"\"\n",
        "    Patch a given part ('attn1', 'attn2', 'ff') of DiT blocks in PixArt-Î±.\n",
        "\n",
        "    Args:\n",
        "        dit: The transformer model\n",
        "        blocks_to_patch: List of block indices to patch\n",
        "        part: 'attn1' (self-attn), 'attn2' (cross-attn), 'ff' (MLP)\n",
        "        mode: 'zero' | 'input' | 'mean'\n",
        "    \"\"\"\n",
        "    for idx in blocks_to_patch:\n",
        "        if idx >= len(dit.transformer_blocks):\n",
        "            print(f\"Warning: Block index {idx} exceeds available blocks ({len(dit.transformer_blocks)})\")\n",
        "            continue\n",
        "\n",
        "        block = dit.transformer_blocks[idx]\n",
        "        sub_module = getattr(block, part, None)\n",
        "\n",
        "        if sub_module is None:\n",
        "            print(f\"Warning: Part '{part}' not found in block {idx}\")\n",
        "            continue\n",
        "\n",
        "        # Store original forward method\n",
        "        if not hasattr(sub_module, '_original_forward'):\n",
        "            sub_module._original_forward = sub_module.forward\n",
        "\n",
        "        def create_ablated_forward(module, ablation_mode):\n",
        "            def ablated_forward(x, *args, **kwargs):\n",
        "                if ablation_mode == 'zero':\n",
        "                    return torch.zeros_like(x)\n",
        "                elif ablation_mode == 'input':\n",
        "                    return x\n",
        "                elif ablation_mode == 'mean':\n",
        "                    return x.mean(dim=-1, keepdim=True).expand_as(x)\n",
        "                else:\n",
        "                    return module._original_forward(x, *args, **kwargs)\n",
        "            return ablated_forward\n",
        "\n",
        "        # Apply the ablation\n",
        "        sub_module.forward = create_ablated_forward(sub_module, mode)\n",
        "        print(f\"Ablated block {idx}, part '{part}' with mode '{mode}'\")\n",
        "\n",
        "def restore_dit_part(dit, blocks_to_restore, part='ff'):\n",
        "    \"\"\"\n",
        "    Restore the original forward method for specified blocks and parts.\n",
        "    \"\"\"\n",
        "    for idx in blocks_to_restore:\n",
        "        if idx >= len(dit.transformer_blocks):\n",
        "            continue\n",
        "\n",
        "        block = dit.transformer_blocks[idx]\n",
        "        sub_module = getattr(block, part, None)\n",
        "\n",
        "        if sub_module is not None and hasattr(sub_module, '_original_forward'):\n",
        "            sub_module.forward = sub_module._original_forward\n",
        "            delattr(sub_module, '_original_forward')\n",
        "            print(f\"Restored block {idx}, part '{part}'\")\n",
        "\n",
        "# Load COCO data\n",
        "with open(COCO_ANN_PATH, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "id2filename = {img['id']: img['file_name'] for img in coco['images']}\n",
        "data = []\n",
        "for ann in coco['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    caption = ann['caption']\n",
        "    img_path = os.path.join(COCO_IMG_DIR, id2filename[img_id])\n",
        "    if os.path.exists(img_path):\n",
        "        data.append((img_path, caption))\n",
        "\n",
        "print(f\"Loaded {len(data)} (image, caption) pairs from COCO val2017.\")\n",
        "\n",
        "# Pick random samples with fixed seed\n",
        "random.seed(42)  # Set seed for reproducibility\n",
        "chosen = random.sample(data, N)\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipe = PixArtAlphaPipeline.from_pretrained(\n",
        "    \"PixArt-alpha/PixArt-XL-2-512x512\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe.to(device)\n",
        "\n",
        "# Load CLIP\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "output_size = (299, 299)\n",
        "os.makedirs(\"./pixart_samples/real\", exist_ok=True)\n",
        "os.makedirs(\"./pixart_samples/fake\", exist_ok=True)\n",
        "os.makedirs(\"./pixart_samples/ablated\", exist_ok=True)\n",
        "\n",
        "# Generate original images\n",
        "print(\"Generating original images...\")\n",
        "for i, (img_path, prompt) in enumerate(tqdm(chosen, desc=\"Generating original\")):\n",
        "    gt_img = Image.open(img_path).convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
        "    gt_img.save(f\"./pixart_samples/real/gt_{i+1}.jpg\")\n",
        "\n",
        "    gen_img = pipe(prompt).images[0].convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
        "    gen_img.save(f\"./pixart_samples/fake/gen_{i+1}.jpg\")\n",
        "\n",
        "# Apply ablation\n",
        "print(f\"Total transformer blocks: {len(pipe.transformer.transformer_blocks)}\")\n",
        "\n",
        "# Example ablations - modify as needed:\n",
        "# ablate_dit_part(pipe.transformer, blocks_to_patch=[0], part='ff', mode='zero')\n",
        "# ablate_dit_part(pipe.transformer, blocks_to_patch=[1, 2], part='attn1', mode='input')\n",
        "ablate_dit_part(pipe.transformer, blocks_to_patch=[0], part='ff', mode='mean')\n",
        "\n",
        "# Generate ablated images\n",
        "print(\"Generating ablated images...\")\n",
        "for i, (img_path, prompt) in enumerate(tqdm(chosen, desc=\"Generating ablated\")):\n",
        "    ablated_img = pipe(prompt).images[0].convert(\"RGB\").resize(output_size, Image.LANCZOS)\n",
        "    ablated_img.save(f\"./pixart_samples/ablated/ablated_{i+1}.jpg\")\n",
        "\n",
        "# Calculate FID\n",
        "print(\"Calculating FID scores...\")\n",
        "print(\"FID (Original vs Real):\")\n",
        "fid_orig = calculate_metrics(\n",
        "    input1=\"./pixart_samples/real\",\n",
        "    input2=\"./pixart_samples/fake\",\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, kid=False, fid=True, verbose=True,\n",
        ")\n",
        "\n",
        "print(\"FID (Ablated vs Real):\")\n",
        "fid_ablate = calculate_metrics(\n",
        "    input1=\"./pixart_samples/real\",\n",
        "    input2=\"./pixart_samples/ablated\",\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, kid=False, fid=True, verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"Original FID: {fid_orig['frechet_inception_distance']:.4f}\")\n",
        "print(f\"Ablated FID: {fid_ablate['frechet_inception_distance']:.4f}\")\n",
        "\n",
        "# Calculate CLIP scores\n",
        "def compute_clip_scores(img_dir, captions, prefix=\"gen\"):\n",
        "    scores = []\n",
        "    for i, (_, prompt) in enumerate(captions):\n",
        "        img = Image.open(f\"{img_dir}/{prefix}_{i+1}.jpg\").convert(\"RGB\")\n",
        "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "        text = clip.tokenize([prompt]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.encode_image(img_tensor)\n",
        "            text_features = clip_model.encode_text(text)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = (image_features @ text_features.T).item()\n",
        "        scores.append(similarity)\n",
        "    return scores\n",
        "\n",
        "print(\"Calculating CLIP scores...\")\n",
        "orig_clip_scores = compute_clip_scores(\"./pixart_samples/fake\", chosen, \"gen\")\n",
        "ablated_clip_scores = compute_clip_scores(\"./pixart_samples/ablated\", chosen, \"ablated\")\n",
        "\n",
        "print(f\"Mean CLIP (Original): {sum(orig_clip_scores)/len(orig_clip_scores):.4f}\")\n",
        "print(f\"Mean CLIP (Ablated): {sum(ablated_clip_scores)/len(ablated_clip_scores):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KaHbo7fDRiiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}